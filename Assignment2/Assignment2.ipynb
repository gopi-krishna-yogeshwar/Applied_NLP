{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d6ea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: contractions in /opt/homebrew/lib/python3.9/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gopi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import classification_report\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "!pip install contractions\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.metrics import classification_report\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d159bb",
   "metadata": {},
   "source": [
    "# Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "610ec8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.tsv\", usecols = ['review_body', 'star_rating'], sep='\\t')\n",
    "df['star_rating'] =  pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df = df.dropna(subset = ['star_rating'])\n",
    "df.loc[:,'star_rating'] = df['star_rating'].astype(int)\n",
    "df.loc[:,'review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "data_set = pd.DataFrame()\n",
    "for i in range(1,6):\n",
    "    temp = df[df['star_rating'] == i].sample(frac=1).reset_index(drop=True).iloc[:20000]\n",
    "    data_set =data_set.append(temp, ignore_index=True)   \n",
    "data_set = data_set.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a2415",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c15b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim\n",
    "google_w2v = gensim.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa67c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pocket'\t'chain'\t-0.03\n",
      "'man'\t'boy'\t0.68\n",
      "'good'\t'best'\t0.55\n",
      "'beads'\t'necklace'\t0.56\n",
      "'ring'\t'diamond'\t0.29\n",
      "'excellent'\t'outstanding'\t0.56\n",
      "'woman'\t'girl'\t0.75\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [('pocket', 'chain'), ('man','boy'),('good','best'),('beads','necklace'),('ring','diamond'),('excellent', 'outstanding'), ('woman','girl')]\n",
    "for w1,w2 in word_pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, google_w2v.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1086857",
   "metadata": {},
   "source": [
    "# Basic pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff44e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#converting reviews to lower cases\n",
    "data_set['review_body'] = data_set['review_body'].str.lower()\n",
    "#extracting content in html tags\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : BeautifulSoup(x).get_text())\n",
    "#removing all urls in the review\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : re.sub(r'http\\S+','',x).strip())\n",
    "#expanding contraction words\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : contractions.fix(x))\n",
    "#removing non alphabetical characters\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : \" \".join(re.sub('[^a-z]+','', word) for word in word_tokenize(x)))\n",
    "#removing extra spaces between words\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : re.sub(' +',' ',x).strip())\n",
    "data_set['review_list'] = data_set['review_body'].apply(lambda x : x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfb0a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "word2vec = gensim.models.Word2Vec(sentences=data_set['review_list'], vector_size=300, min_count=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50f18d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pocket'\t'chain'\t0.07\n",
      "'man'\t'boy'\t0.48\n",
      "'good'\t'best'\t0.38\n",
      "'beads'\t'necklace'\t0.23\n",
      "'ring'\t'diamond'\t0.27\n",
      "'excellent'\t'outstanding'\t0.71\n",
      "'woman'\t'girl'\t0.72\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [('pocket', 'chain'), ('man','boy'),('good','best'),('beads','necklace'),('ring','diamond'), ('excellent', 'outstanding'),('woman','girl')]\n",
    "for w1,w2 in word_pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, word2vec.wv.similarity(w1,w2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99249637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic similarity between king - man + woman = king is : 0.7300517\n",
      "semantic similarity between queen - woman + man = king is : 0.70464075\n",
      "semantic similarity between beads + thread = necklace is : 0.5135838\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "king = google_w2v['king']\n",
    "man = google_w2v['man']\n",
    "woman = google_w2v['woman']\n",
    "resulting_vec = king-man+woman\n",
    "new_vec = google_w2v['queen'] - woman + man\n",
    "beads= google_w2v['beads']\n",
    "thread = google_w2v['thread']\n",
    "necklace_vec = beads + thread\n",
    "print(\"semantic similarity between king - man + woman = king is :\",np.dot(resulting_vec, google_w2v['queen'])/(norm(resulting_vec)*norm(google_w2v['queen'])))\n",
    "print(\"semantic similarity between queen - woman + man = king is :\",np.dot(new_vec, google_w2v['king'])/(norm(new_vec)*norm(google_w2v['king'])))\n",
    "print(\"semantic similarity between beads + thread = necklace is :\",np.dot(necklace_vec, google_w2v['necklace'])/(norm(necklace_vec)*norm(google_w2v['necklace'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9418f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic similarity between king - man + woman = king is : 0.29035768\n",
      "semantic similarity between queen - woman + man = king is : -0.050481144\n",
      "semantic similarity between beads + thread = necklace is : 0.2400201\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "king = word2vec.wv['king']\n",
    "man = word2vec.wv['man']\n",
    "woman = word2vec.wv['woman']\n",
    "resulting_vec = king-man+woman\n",
    "new_vec = word2vec.wv['queen'] - woman + man\n",
    "beads= word2vec.wv['beads']\n",
    "thread = word2vec.wv['thread']\n",
    "necklace_vec = beads + thread\n",
    "print(\"semantic similarity between king - man + woman = king is :\",np.dot(resulting_vec, word2vec.wv['queen'])/(norm(resulting_vec)*norm(word2vec.wv['queen'])))\n",
    "print(\"semantic similarity between queen - woman + man = king is :\",np.dot(new_vec, word2vec.wv['king'])/(norm(new_vec)*norm(word2vec.wv['king'])))\n",
    "print(\"semantic similarity between beads + thread = necklace is :\",np.dot(necklace_vec, word2vec.wv['necklace'])/(norm(necklace_vec)*norm(word2vec.wv['necklace'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c33a1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import KeyedVectors\n",
    "#word_vectors = word_2_vec\n",
    "#word_vectors.save(\"word2vec.wordvectors\")\n",
    "#google_w2v = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb4ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(report):\n",
    "  a = \"accuracy is: \" + str(report['accuracy']) + \"\\n\"\n",
    "  p = \"precision is: \" + str(report['macro avg']['precision']) + \"\\n\"\n",
    "  r = \"recall is: \" + str(report['macro avg']['recall']) + \"\\n\"\n",
    "  f1 = \"F1 score is: \" + str(report['macro avg']['f1-score']) + \"\\n\"\n",
    "  return p+r+f1+a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7c9f",
   "metadata": {},
   "source": [
    "# Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a40577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.019232177734375, 0.04854736328125, 0.044506...\n",
       "1        [-0.0034208943073014567, 0.027732433233045993,...\n",
       "2        [0.12060546875, 0.118896484375, -0.05334472656...\n",
       "3        [0.020255088806152344, 0.036087870597839355, 0...\n",
       "4        [-0.000396728515625, -7.926093207465278e-05, 0...\n",
       "                               ...                        \n",
       "99995    [-0.04798719618055555, 0.003268771701388889, 0...\n",
       "99996    [0.1359375, -0.027951431274414063, -0.04521484...\n",
       "99997    [0.026754483309659093, 0.013201210715553977, 0...\n",
       "99998    [0.001480712890625, 0.01921142578125, 0.013985...\n",
       "99999    [0.028919677734375, 0.04644935607910156, 0.070...\n",
       "Name: review_list, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_set = []\n",
    "def get_avg_word2vec(x):\n",
    "  #print(x)\n",
    "  count = 0\n",
    "  add_vector = np.zeros(300)\n",
    "  for word in x:\n",
    "        if word in google_w2v:\n",
    "            add_vector = np.add(add_vector, google_w2v[word])\n",
    "            count += 1\n",
    "  if count > 0:\n",
    "    add_vector = np.divide(add_vector, len(x))\n",
    "  global_set.append(add_vector)   \n",
    "  return add_vector\n",
    "    \n",
    "\n",
    "data_set['review_list'].apply(lambda x: get_avg_word2vec(x))\n",
    "#data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d66d298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(global_set, data_set['star_rating'], test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56e151",
   "metadata": {},
   "source": [
    "# Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88384f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.467763105616742\n",
      "recall is: 0.3737731280706565\n",
      "F1 score is: 0.3371223090952351\n",
      "accuracy is: 0.37155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(x_train, y_train)\n",
    "y_test_pred_perceptron = perceptron_model.predict(x_test)\n",
    "report = classification_report(y_test, y_test_pred_perceptron,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06806a61",
   "metadata": {},
   "source": [
    "# Linear SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d660621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.4651957853936747\n",
      "recall is: 0.48321042971229333\n",
      "F1 score is: 0.46348707325528676\n",
      "accuracy is: 0.48505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "y_test_pred_svm = svm_classifier.predict(x_test)\n",
    "report=classification_report(y_test, y_test_pred_svm,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c3511",
   "metadata": {},
   "source": [
    "# Question 4a: MLP with average word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e329787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functions\n",
    "\n",
    "class LoadDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    self.x_train = torch.tensor(data[0], dtype = torch.float32)\n",
    "    self.y_train = torch.tensor(data[1].to_numpy(), dtype = torch.long)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c121817",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 300\n",
    "hidden_1 = 50\n",
    "hidden_2 = 10\n",
    "output = 5\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(NeuralNet, self).__init__()\n",
    "\n",
    "    self.layer_1 = nn.Linear(input_nodes, hidden_1)\n",
    "    self.layer_2 = nn.Linear(hidden_1, hidden_2)\n",
    "    self.layer_3 = nn.Linear(hidden_2, output)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x = x.view(-1, input)\n",
    "    x = Functions.relu(self.layer_1(x))\n",
    "    x = self.dropout(x)\n",
    "    x = Functions.relu(self.layer_2(x))\n",
    "    x = self.dropout(x)\n",
    "    x = self.layer_3(x)\n",
    "    return x\n",
    "\n",
    "  def set_dropout(self, dp):\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "mlp_model = NeuralNet()\n",
    "#print(mlp_model)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71706195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/1734367578.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  self.x_train = torch.tensor(data[0], dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "def train_validation_split(x_train, y_train):\n",
    "  x_validation = x_train[0:int(0.2*len(x_train))]\n",
    "  y_validation = y_train[0:int(0.2*len(y_train))]\n",
    "  x_train = x_train[int(0.2*len(x_train)):len(x_train)]\n",
    "  y_train = y_train[int(0.2*len(y_train)):len(y_train)]\n",
    "  return x_train, y_train, x_validation, y_validation\n",
    "\n",
    "def train_network(model, train_loader, validation_loader, n_epochs):\n",
    "  validation_loss_min = np.Inf\n",
    "  for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for index, (data,label) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data)\n",
    "      loss = criteria(output, label-1)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()*data.size(0)\n",
    "  \n",
    "    model.eval()\n",
    "    for index, (data, label) in enumerate(validation_loader):\n",
    "      output = model(data)\n",
    "      loss = criteria(output, label-1)\n",
    "      validation_loss += loss.item()*data.size(0)\n",
    "\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    validation_loss = validation_loss/len(validation_loader.dataset)\n",
    "    if validation_loss <= validation_loss_min:\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        validation_loss_min = validation_loss  \n",
    "\n",
    "x_train, y_train, x_validation, y_validation = train_validation_split(x_train, y_train)\n",
    "\n",
    "train_dataset = LoadDataset((x_train, y_train))\n",
    "validation_dataset = LoadDataset((x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "308accb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "mlp_train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "mlp_validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "train_network(mlp_model, mlp_train_loader, mlp_validation_loader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ad3a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.49004611709622187\n",
      "recall is: 0.5026240744646133\n",
      "F1 score is: 0.4917581448876289\n",
      "accuracy is: 0.5032\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/3416720493.py:14: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  predictions = np.array(predictions)\n",
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/3416720493.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  predictions = np.array(predictions)\n"
     ]
    }
   ],
   "source": [
    "mlp_model.load_state_dict(torch.load('model.pt'))\n",
    "test_dataset = LoadDataset((x_test,y_test))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    for index, (data, label) in enumerate(dataloader):\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(predicted.cpu())\n",
    "    return prediction_list\n",
    "\n",
    "predictions = predict(mlp_model,test_loader)\n",
    "predictions = np.array(predictions)\n",
    "predictions_int = []\n",
    "for pred in predictions:\n",
    "  predictions_int = np.append(predictions_int, int(pred) + 1)\n",
    "report=classification_report(y_test, predictions_int,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d415214",
   "metadata": {},
   "source": [
    "# Question 4b: MLP with concatenated word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8938b7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.084472656, -0.0003528595, 0.053222656, 0.09...\n",
       "1        [0.10107422, -0.0038146973, 0.018188477, 0.129...\n",
       "2        [-0.22558594, -0.01953125, 0.09082031, 0.23730...\n",
       "3        [0.080078125, 0.10498047, 0.049804688, 0.05346...\n",
       "4        [0.084472656, -0.0003528595, 0.053222656, 0.09...\n",
       "                               ...                        \n",
       "99995    [-0.016235352, 0.09423828, 0.091796875, 0.1196...\n",
       "99996    [0.109375, 0.140625, -0.03173828, 0.16601562, ...\n",
       "99997    [0.17089844, 0.024291992, 0.13867188, 0.022216...\n",
       "99998    [0.109375, 0.140625, -0.03173828, 0.16601562, ...\n",
       "99999    [0.14453125, 0.04711914, 0.10058594, 0.328125,...\n",
       "Name: review_list, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_vectors_concat = []\n",
    "def get_vectors_concat(words):\n",
    "  result = []\n",
    "  count = 0\n",
    "  for word in words:\n",
    "    if word in google_w2v and count < 10:\n",
    "      result.extend(google_w2v[word])\n",
    "      count+=1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "  diff = 3000 - len(result)\n",
    "  result.extend(np.zeros(diff))\n",
    "  mlp_vectors_concat.append(result)\n",
    "  return result\n",
    "\n",
    "data_set['review_list'].apply(lambda x: get_vectors_concat(x))\n",
    "#data_set['review_vectors'] = data_set['review_vectors'].apply(lambda x : np.array(x))\n",
    "#print(data_set.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d99a3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "x_train,x_test,y_train,y_test = train_test_split(mlp_vectors_concat, data_set['star_rating'], test_size = 0.2)\n",
    "x_train, y_train, x_validation, y_validation = train_validation_split(x_train, y_train)\n",
    "#print(len(x_train))\n",
    "train_dataset = LoadDataset((x_train, y_train))\n",
    "validation_dataset = LoadDataset((x_validation, y_validation))\n",
    "test_dataset = LoadDataset((x_test,y_test))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "289675c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 3000\n",
    "hidden_1 = 50\n",
    "hidden_2 = 10\n",
    "output = 5\n",
    "\n",
    "mlp_concat_model = NeuralNet()\n",
    "mlp_concat_model.set_dropout(0.5)\n",
    "#print(mlp_concat_model)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_concat_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train_network(mlp_concat_model, train_loader, validation_loader, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba2298b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.4106313941791613\n",
      "recall is: 0.4278203623274702\n",
      "F1 score is: 0.4108242828066257\n",
      "accuracy is: 0.4282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/2232472655.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  predictions = np.array(predictions)\n",
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/2232472655.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  predictions = np.array(predictions)\n"
     ]
    }
   ],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    for index, (data, label) in enumerate(dataloader):\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(predicted.cpu())\n",
    "    return prediction_list\n",
    "\n",
    "predictions = predict(mlp_concat_model,test_loader)\n",
    "predictions = np.array(predictions)\n",
    "predictions_int = []\n",
    "for pred in predictions:\n",
    "  predictions_int = np.append(predictions_int, int(pred) + 1)\n",
    "report=classification_report(y_test, predictions_int,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dbbe73",
   "metadata": {},
   "source": [
    "# Question 5a: Basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663f2dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.084472656, -0.0003528595, 0.053222656, 0.09...\n",
       "1        [0.10107422, -0.0038146973, 0.018188477, 0.129...\n",
       "2        [-0.22558594, -0.01953125, 0.09082031, 0.23730...\n",
       "3        [0.080078125, 0.10498047, 0.049804688, 0.05346...\n",
       "4        [0.084472656, -0.0003528595, 0.053222656, 0.09...\n",
       "                               ...                        \n",
       "99995    [-0.016235352, 0.09423828, 0.091796875, 0.1196...\n",
       "99996    [0.109375, 0.140625, -0.03173828, 0.16601562, ...\n",
       "99997    [0.17089844, 0.024291992, 0.13867188, 0.022216...\n",
       "99998    [0.109375, 0.140625, -0.03173828, 0.16601562, ...\n",
       "99999    [0.14453125, 0.04711914, 0.10058594, 0.328125,...\n",
       "Name: review_list, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rnn_vectors = []\n",
    "def get_vectors(words):\n",
    "  result = []\n",
    "  count = 0\n",
    "  for word in words:\n",
    "    if word in google_w2v and count < 20:\n",
    "      result.extend(google_w2v[word])\n",
    "      count+=1\n",
    "    if count > 20:\n",
    "        break\n",
    "\n",
    "  diff = 6000 - len(result)\n",
    "  result.extend(np.zeros(diff))\n",
    "  global_rnn_vectors.append(result)\n",
    "  return result\n",
    "data_set['review_list'].apply(lambda x: get_vectors(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9874d86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x in global_rnn_vectors:\n",
    "    if len(x) != 6000:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0b4d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(global_rnn_vectors, data_set['star_rating'], test_size = 0.2)\n",
    "x_train, y_train, x_validation, y_validation = train_validation_split(x_train, y_train)\n",
    "\n",
    "train_dataset = LoadDataset((x_train, y_train))\n",
    "validation_dataset = LoadDataset((x_validation, y_validation))\n",
    "test_dataset = LoadDataset((x_test,y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle = True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=100, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d74d251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(300, 20, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functions\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        out,_ = self.rnn(x,h0)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "n_hidden = 20\n",
    "rnn_model = RNN(300, 20, 2, 5)\n",
    "print(rnn_model)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn_model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "438bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "\n",
    "n_epochs = 100\n",
    "validation_loss_min = np.Inf\n",
    "for epoch in range(n_epochs):\n",
    "  train_loss = 0.0\n",
    "  validation_loss = 0.0\n",
    "  for index, (data,label) in enumerate(train_loader):\n",
    "    data = data.reshape(-1, 20, 300)\n",
    "    #print(data.shape)\n",
    "    outputs = rnn_model(data)\n",
    "    \n",
    "    loss = criteria(outputs, label-1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "  \n",
    "    rnn_model.eval()\n",
    "  for index, (data, label) in enumerate(validation_loader):\n",
    "      data = data.reshape(-1, 20, 300)\n",
    "      output = rnn_model(data)\n",
    "      loss = criteria(output, label-1)\n",
    "      validation_loss += loss.item()*data.size(0)\n",
    "\n",
    "    \n",
    "  train_loss = train_loss/len(train_loader.dataset)\n",
    "  validation_loss = validation_loss/len(validation_loader.dataset)\n",
    "  if validation_loss <= validation_loss_min:\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pt')\n",
    "        validation_loss_min = validation_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "835c16bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.45601098554727565\n",
      "recall is: 0.4732367548006525\n",
      "F1 score is: 0.45753562317164176\n",
      "accuracy is: 0.47115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/2842718222.py:14: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  rnn_predictions = np.array(rnn_sample_pred)\n",
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/2842718222.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  rnn_predictions = np.array(rnn_sample_pred)\n"
     ]
    }
   ],
   "source": [
    "rnn_model.load_state_dict(torch.load('rnn_model.pt'))\n",
    "rnn_sample_pred = []\n",
    "with torch.no_grad():\n",
    "  for index, (data,label) in enumerate(test_loader):\n",
    "    data = data.reshape(-1, 20, 300)\n",
    "    #print(data.shape)\n",
    "    outputs = rnn_model(data)\n",
    "\n",
    "    _,predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    rnn_sample_pred.append(predicted)\n",
    "\n",
    "\n",
    "rnn_predictions = np.array(rnn_sample_pred)\n",
    "predictions_int = []\n",
    "for pred in rnn_predictions:\n",
    "  predictions_int = np.append(predictions_int, int(pred) + 1)\n",
    "report=classification_report(y_test, predictions_int,output_dict=True)\n",
    "print(print_metrics(report))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c20f84",
   "metadata": {},
   "source": [
    "# Question 5b: Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bbd29e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (rnn): GRU(300, 20, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functions\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        out,_ = self.rnn(x,h0)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "n_hidden = 20\n",
    "gru_model = GRU(300, 20, 2, 5)\n",
    "print(gru_model)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(gru_model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f030716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(global_rnn_vectors, data_set['star_rating'], test_size = 0.2)\n",
    "x_train, y_train, x_validation, y_validation = train_validation_split(x_train, y_train)\n",
    "\n",
    "train_dataset = LoadDataset((x_train, y_train))\n",
    "validation_dataset = LoadDataset((x_validation, y_validation))\n",
    "test_dataset = LoadDataset((x_test,y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle = True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=50, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e953db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is: 380.6278429031372\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "n_epochs = 50\n",
    "validation_loss_min = np.Inf\n",
    "for epoch in range(n_epochs):\n",
    "  train_loss = 0.0\n",
    "  validation_loss = 0.0\n",
    "  for index, (data,label) in enumerate(train_loader):\n",
    "    data = data.reshape(-1, 20, 300)\n",
    "    #print(data.shape)\n",
    "    outputs = gru_model(data)\n",
    "    \n",
    "    loss = criteria(outputs, label-1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "  \n",
    "    gru_model.eval()\n",
    "  for index, (data, label) in enumerate(validation_loader):\n",
    "      data = data.reshape(-1, 20, 300)\n",
    "      output = gru_model(data)\n",
    "      loss = criteria(output, label-1)\n",
    "      validation_loss += loss.item()*data.size(0)\n",
    "\n",
    "    \n",
    "  train_loss = train_loss/len(train_loader.dataset)\n",
    "  validation_loss = validation_loss/len(validation_loader.dataset)\n",
    "    \n",
    "  if validation_loss <= validation_loss_min:\n",
    "        torch.save(gru_model.state_dict(), 'gru_model.pt')\n",
    "        validation_loss_min = validation_loss  \n",
    "\n",
    "print(\"Time taken is:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4be6fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is: 0.49368513424744964\n",
      "recall is: 0.5027778621641076\n",
      "F1 score is: 0.4957633845981797\n",
      "accuracy is: 0.50255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/2153500255.py:14: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  rnn_predictions = np.array(rnn_sample_pred)\n",
      "/var/folders/y9/k2kkzqkj49jcv8lx3vwms0140000gn/T/ipykernel_676/2153500255.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  rnn_predictions = np.array(rnn_sample_pred)\n"
     ]
    }
   ],
   "source": [
    "gru_model.load_state_dict(torch.load('gru_model.pt'))\n",
    "rnn_sample_pred = []\n",
    "with torch.no_grad():\n",
    "  for index, (data,label) in enumerate(test_loader):\n",
    "    data = data.reshape(-1, 20, 300)\n",
    "    #print(data.shape)\n",
    "    outputs = gru_model(data)\n",
    "\n",
    "    _,predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    rnn_sample_pred.append(predicted)\n",
    "\n",
    "\n",
    "rnn_predictions = np.array(rnn_sample_pred)\n",
    "predictions_int = []\n",
    "for pred in rnn_predictions:\n",
    "  predictions_int = np.append(predictions_int, int(pred) + 1)\n",
    "report=classification_report(y_test, predictions_int,output_dict=True)\n",
    "print(print_metrics(report))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ce4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
